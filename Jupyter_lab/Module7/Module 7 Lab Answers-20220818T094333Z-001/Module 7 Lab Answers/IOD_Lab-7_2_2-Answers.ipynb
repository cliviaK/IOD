{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeJGLuogrb1R"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYkcntb_rb1V"
   },
   "source": [
    "# Lab 7.2.2: Boosting\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "- Read the guides and hints then create the necessary analysis and code to find an answer and conclusion for the scenario below.\n",
    "- The baseline results (minimum) are:\n",
    "    - **Accuracy** = 0.9429\n",
    "    - **ROC AUC**  = 0.9333\n",
    "- Try to achieve better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63GzlBXnrb1Y"
   },
   "source": [
    "# Foreword\n",
    "It is common that companies and professionals start with the data immediately available. Although this approach works, ideally the first step is to identify the problem or question and only then identify and obtain the set of data that can help to solve or answer the problem.\n",
    "\n",
    "Also, given the current abundance of data, processing power and some particular machine learning methods, there could be a temptation to use ALL the data available. **Quality** is _**better**_ than **Quantity**!\n",
    "\n",
    "Part of calling this discipline **Data Science** is that it is supposed to follow a process and not reach conclusions without support from evidence.\n",
    "\n",
    "Moreover, it is a creative, exploratory, laborious, iterative and interactive process. It is part of the process to repeat, review and change when finding a dead-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3KrWgu0rb1c"
   },
   "source": [
    "## Scenario: Predicting Breast Cancer\n",
    "The dataset you are going to be using for this laboratory is popularly known as the **Wisconsin Breast Cancer** dataset. The task related to it is Classification.\n",
    "\n",
    "The dataset contains a total number of _10_ features labelled in either **benign** or **malignant** classes. The features have _699_ instances out of which _16_ feature values are missing. The dataset only contains numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OdL_rH3rb1e"
   },
   "source": [
    "# Step 1: Define the problem or question\n",
    "Identify the subject matter and the given or obvious questions that would be relevant in the field.\n",
    "\n",
    "## Potential Questions\n",
    "List the given or obvious questions.\n",
    "\n",
    "## Actual Question\n",
    "Choose the **one** question that should be answered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niz9ABvtrb1h"
   },
   "source": [
    "# Step 1 (Answer): Define the problem or question\n",
    "\n",
    "## Potential Questions\n",
    "- Will a person be diagnosed with a **Benign** or **Malignant** cancer.\n",
    "- Other\n",
    "\n",
    "## Actual Question\n",
    "- Will a person be diagnosed with a **Benign** or **Malignant** cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4kLCtzvrb1j"
   },
   "source": [
    "# Step 2: Find the Data\n",
    "### Wisconsin Breast Cancer DataSet\n",
    "- **Citation Request**\n",
    "\n",
    "    This breast cancer databases was obtained from the **University of Wisconsin Hospitals**, **Madison** from **Dr. William H. Wolberg**. If you publish results when using this database, then please include this information in your acknowledgements.\n",
    "\n",
    "- **Title**\n",
    "\n",
    "    Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "- **Sources**\n",
    "    - **Creator**\n",
    "            Dr. William H. Wolberg (physician)\n",
    "            University of Wisconsin Hospitals\n",
    "            Madison, Wisconsin\n",
    "            USA\n",
    "    - **Donor**\n",
    "            Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "            Received by David W. Aha (aha@cs.jhu.edu)\n",
    "    - **Date**\n",
    "            15 July 1992\n",
    "        \n",
    "### UCI - Machine Learning Repository\n",
    "- Center for Machine Learning and Intelligent Systems\n",
    "\n",
    "The [**UCI Machine Learning Repository**](http://archive.ics.uci.edu/ml/about.html) is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSRUsWJQrb1l"
   },
   "source": [
    "# Step 2 (Answer): Find the Data\n",
    "[Breast Cancer Wisconsin (Original) Data Set](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original))\n",
    "\n",
    "## Title: Breast Cancer Wisconsin (Original) DataSet\n",
    "[Link](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names)\n",
    "\n",
    "- **Citation Request**\n",
    "\n",
    "    This breast cancer databases was obtained from the **University of Wisconsin Hospitals**, **Madison** from **Dr. William H. Wolberg**. If you publish results when using this database, then please include this information in your acknowledgements.\n",
    "\n",
    "- **Title**\n",
    "\n",
    "    Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "- **Sources**\n",
    "    - Creator:\n",
    "            Dr. William H. Wolberg (physician)\n",
    "            University of Wisconsin Hospitals\n",
    "            Madison, Wisconsin\n",
    "            USA\n",
    "    - Donor:\n",
    "            Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "            Received by David W. Aha (aha@cs.jhu.edu)\n",
    "    - Date:\n",
    "            15 July 1992\n",
    "\n",
    "- **Past Usage**\n",
    "\n",
    "    Attributes 2 through 10 have been used to represent instances.\n",
    "    Each instance has one of 2 possible classes: benign or malignant.\n",
    "\n",
    "    1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. In _Proceedings of the National Academy of Sciences_, _87_, 9193--9196.\n",
    "        - Size of data set: only 369 instances (at that point in time)\n",
    "        - Collected classification results: 1 trial only\n",
    "        - Two pairs of parallel hyperplanes were found to be consistent with 50% of the data\n",
    "            - Accuracy on remaining 50% of dataset: 93.5%\n",
    "        - Three pairs of parallel hyperplanes were found to be consistent with 67% of data\n",
    "            - Accuracy on remaining 33% of dataset: 95.9%\n",
    "\n",
    "    2. Zhang,~J. (1992). Selecting typical instances in instance-based learning. In _Proceedings of the Ninth International Machine Learning Conference_ (pp. 470-479). Aberdeen, Scotland: Morgan Kaufmann.\n",
    "        - Size of data set: only 369 instances (at that point in time)\n",
    "        - Applied 4 instance-based learning algorithms \n",
    "        - Collected classification results averaged over 10 trials\n",
    "        - Best accuracy result: \n",
    "            - 1-nearest neighbor: 93.7%\n",
    "            - trained on 200 instances, tested on the other 169\n",
    "        - Also of interest:\n",
    "            - Using only typical instances: 92.2% (storing only 23.1 instances)\n",
    "            - trained on 200 instances, tested on the other 169\n",
    "\n",
    "- **Relevant Information**\n",
    "\n",
    "    Samples arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore reflects this chronological grouping of the data. This grouping information appears immediately below, having been removed from the data itself:\n",
    "\n",
    "        Group 1: 367 instances (January 1989)\n",
    "        Group 2:  70 instances (October 1989)\n",
    "        Group 3:  31 instances (February 1990)\n",
    "        Group 4:  17 instances (April 1990)\n",
    "        Group 5:  48 instances (August 1990)\n",
    "        Group 6:  49 instances (Updated January 1991)\n",
    "        Group 7:  31 instances (June 1991)\n",
    "        Group 8:  86 instances (November 1991)\n",
    "        ---------------------------------------------------------------\n",
    "        Total:   699 points (as of the donated datbase on 15 July 1992)\n",
    "\n",
    "    Note that the results summarized above in Past Usage refer to a dataset of size 369, while Group 1 has only 367 instances. This is because it originally contained 369 instances; 2 were removed. The following statements summarizes changes to the original Group 1's set of data:\n",
    "\n",
    "        #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
    "        #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
    "        #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
    "        #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
    "        #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
    "        #####                  : Changed 0 to 1 in field 8 of following sample:\n",
    "        #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
    "\n",
    "- **Number of Instances**\n",
    "    - 699 (as of 15 July 1992)\n",
    "\n",
    "- **Number of Attributes**\n",
    "    - 10 plus the class attribute\n",
    "\n",
    "- **Attribute Information**\n",
    "\n",
    "    Class attribute has been moved to last column\n",
    "\n",
    "         #  Attribute                   Domain\n",
    "        --- --------------------------- -----------\n",
    "         1. Sample code number          id number\n",
    "         2. Clump Thickness             1 - 10\n",
    "         3. Uniformity of Cell Size     1 - 10\n",
    "         4. Uniformity of Cell Shape    1 - 10\n",
    "         5. Marginal Adhesion           1 - 10\n",
    "         6. Single Epithelial Cell Size 1 - 10\n",
    "         7. Bare Nuclei                 1 - 10\n",
    "         8. Bland Chromatin             1 - 10\n",
    "         9. Normal Nucleoli             1 - 10\n",
    "        10. Mitoses                     1 - 10\n",
    "        11. Class                       (2 for benign, 4 for malignant)\n",
    "\n",
    "- **Missing attribute values**\n",
    "    - 16\n",
    "\n",
    "    There are 16 instances in Groups 1 to 6 that contain a single missing (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
    "\n",
    "- **Class distribution**\n",
    "    - Benign: 458 (65.5%)\n",
    "    - Malignant: 241 (34.5%)\n",
    "    \n",
    "- **Data**\n",
    "    - [Download](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQKJ0WfLrb1m"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1lYxc4Krb1s"
   },
   "source": [
    "# Step 3: Read the Data\n",
    "- Read the data\n",
    "- Perform some basic structural cleaning to facilitate the work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tcy9-sE3rb1u"
   },
   "source": [
    "# Step 3 (Answer): Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDr4vfIJrb1x"
   },
   "outputs": [],
   "source": [
    "## reading and inspect the data\n",
    "\n",
    "columns = [\n",
    "    'Sample_Number',\n",
    "    'Clump_Thickness',\n",
    "    'Uniformity_of_Cell_Size',\n",
    "    'Uniformity_of_Cell_Shape',\n",
    "    'Marginal_Adhesion',\n",
    "    'Single_Epithelial_Cell_Size',\n",
    "    'Bare_Nuclei',\n",
    "    'Bland_Chromatin',\n",
    "    'Normal_Nucleoli',\n",
    "    'Mitoses',\n",
    "    'Class'\n",
    "]\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer = 'breast-cancer-wisconsin.csv',\n",
    "    header = None,\n",
    "    names = columns,\n",
    "    usecols = columns[1:], # do not use the first column\n",
    "    na_values = '?' # convert the '?' to NA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKK9LZnKrb11"
   },
   "outputs": [],
   "source": [
    "# get more information on the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdGElbqarb14",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG-76bR8rb17"
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieYL2AQQrb19"
   },
   "source": [
    "# Step 4: Explore and Clean the Data\n",
    "- Perform some initial simple **EDA** (Exploratory Data Analysis)\n",
    "- Check for\n",
    "    - **Number of features**\n",
    "    - **Data types**\n",
    "    - **Domains, Intervals**\n",
    "    - **Outliers** (are they valid or expurious data [read or measure errors])\n",
    "    - **Null** (values not present or coded [as zero of empty strings])\n",
    "    - **Missing Values** (coded [as zero of empty strings] or values not present)\n",
    "    - **Coded content** (classes identified by numbers or codes to represent absence of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gdobT6Srb1-"
   },
   "source": [
    "# Step 4 (Answer): Explore and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "708pOcc8rb1_"
   },
   "outputs": [],
   "source": [
    "# Domains, Intervals\n",
    "print('- Domains, Intervals')\n",
    "for c in df.columns:\n",
    "    x = df[c].unique()\n",
    "    x.sort()\n",
    "    print('+ %-27s: (%-s)' % (c, df[c].dtypes.name))\n",
    "\n",
    "    if df[c].dtypes.name != 'object':\n",
    "        print('  min: %d, max: %d' % (df[c].min(), df[c].max()))\n",
    "    print('  values: %s' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfI-U0whrb2B"
   },
   "outputs": [],
   "source": [
    "print(df['Bare_Nuclei'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUnbALrurb2D"
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame object into NumPy array otherwise you will not be able to impute\n",
    "values = df.values\n",
    "\n",
    "# define the criteria for dealing with the missing values\n",
    "imputer = SimpleImputer(\n",
    "    missing_values = np.nan,\n",
    "    strategy = 'median'\n",
    ")\n",
    "# Now impute it\n",
    "imputedData = imputer.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKUjmkZKrb2G"
   },
   "source": [
    "The ranges of the features of the dataset are not the same. This may cause a problem. \n",
    "\n",
    "Normalisation (convert all ranges to `[0-1]`) can address this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNW4oQjArb2H"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "normalizedData = scaler.fit_transform(imputedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-8XJ5WSrb2J",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move the data back to a dataframe\n",
    "df_n = pd.DataFrame.from_records(normalizedData, columns = columns[1:])\n",
    "df_n['Class'] = df_n['Class'].astype(np.int8)\n",
    "df_n.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xuST-ktrb2L"
   },
   "outputs": [],
   "source": [
    "# Check for Outliers\n",
    "for c in df_n.columns:\n",
    "    fig, ax = plt.subplots(2, \n",
    "                           figsize = (10, 5),\n",
    "                           sharex = True, \n",
    "                           gridspec_kw = {'height_ratios': (0.15, 0.85)})\n",
    "\n",
    "    ax_box = ax[0]\n",
    "    ax_box = sns.boxplot(df_n[c], ax = ax_box)\n",
    "    ax_box.set(title = c, yticks = [], xlabel = '')\n",
    "    sns.despine(ax = ax_box, left = True)\n",
    "\n",
    "    ax_hist = ax[1]\n",
    "    ax_hist = sns.distplot(df_n[c], ax = ax_hist)\n",
    "    ax_hist.set(xlabel = '')\n",
    "    sns.despine(ax = ax_hist)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80DQoL0frb2O"
   },
   "outputs": [],
   "source": [
    "# create X and y to match Scikit-Learn parlance\n",
    "\n",
    "features = columns[1:-1] # discard the first and last columns\n",
    "outcome = 'Class'\n",
    "\n",
    "# X include all the features\n",
    "X = df_n[features].copy()\n",
    "# y is the target variable\n",
    "y = df_n[outcome].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4ADRbQMrb2Q"
   },
   "outputs": [],
   "source": [
    "## Check the data\n",
    "\n",
    "# About X\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wi8VDeajrb2S"
   },
   "outputs": [],
   "source": [
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYN4bV20rb2U"
   },
   "outputs": [],
   "source": [
    "# About y\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TegLkw2Vrb2W"
   },
   "outputs": [],
   "source": [
    "y.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMOBJGA6rb2X"
   },
   "source": [
    "## Pearson Correlation Heatmap\n",
    "Check correlation of the features to see how related one feature is to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QccQr7AXrb2Y"
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.title('Pearson Correlation of Features', size = 15)\n",
    "sns.heatmap(df_n.astype(float).corr(),\n",
    "            linewidths = 0.1,\n",
    "            vmax = 1.0,\n",
    "            square = True,\n",
    "            cmap = colormap,\n",
    "            linecolor = 'white',\n",
    "            annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDV6g68xrb2a"
   },
   "outputs": [],
   "source": [
    "## Visualise the data points\n",
    "\n",
    "# visualise features in pairs\n",
    "sns.pairplot(df_n)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfcO8alxrb2d"
   },
   "source": [
    "# Step 5: Prepare the Data\n",
    "- Deal with the data as required by the modelling technique\n",
    "    - **Outliers** (remove or adjust if possible or necessary)\n",
    "    - **Null** (remove or interpolate if possible or necessary)\n",
    "    - **Missing Values** (remove or interpolate if possible or necessary)\n",
    "    - **Coded content** (transform if possible or necessary [str to number or vice-versa])\n",
    "    - **Normalisation** (if possible or necessary)\n",
    "    - **Feature Engeneer** (if useful or necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaEHga2frb2e"
   },
   "source": [
    "# Step 5 (Answer): Prepare the Data\n",
    "Varies as per each problem and modelling.\n",
    "\n",
    "No especific action is necessary initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG056Ajjrb2h"
   },
   "source": [
    "# Step 6: Modelling\n",
    "Refer to the Problem and Main Question.\n",
    "- What are the input variables (features)?\n",
    "- Is there an output variable (label)?\n",
    "- If there is an output variable:\n",
    "    - What is it?\n",
    "    - What is its type?\n",
    "- What type of Modelling is it?\n",
    "    - [ ] Supervised\n",
    "    - [ ] Unsupervised \n",
    "- What type of Modelling is it?\n",
    "    - [ ] Regression\n",
    "    - [ ] Classification (binary) \n",
    "    - [ ] Classification (multi-class)\n",
    "    - [ ] Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrqUdxwOrb2i"
   },
   "source": [
    "# Step 6 (Answer): Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjE_TOGmrb2i"
   },
   "outputs": [],
   "source": [
    "print('- What are the input variables (features)?')\n",
    "print('  - %s' % ', '.join(features))\n",
    "print('- Is there an output variable (label)?')\n",
    "print('  - %s' % ('Yes' if outcome else 'No'))\n",
    "print('- If there is an output variable:')\n",
    "print('    - Which one is it?')\n",
    "print('      - %s' % outcome)\n",
    "print('    - What is its type?')\n",
    "print('      - %s' % y.dtypes)\n",
    "print('  - What type of Modelling is it?')\n",
    "print('    - [%s] Supervised' % ('x' if outcome else ' '))\n",
    "print('    - [%s] Unsupervised' % (' ' if outcome else 'x'))\n",
    "print('  - What type of Modelling is it?')\n",
    "print('    - [%s] Regression' % ('x' if (y.dtypes != 'object') & (len(y.unique()) >= 20) else ' '))\n",
    "print('    - [%s] Classification (binary)' % ('x' if len(y.unique()) == 2 else ' '))\n",
    "print('    - [%s] Classification (multi-class)' % ('x' if (y.dtypes == 'object') and (len(y.unique()) != 2) else ' '))\n",
    "print('    - [%s] Clustering' % (' ' if outcome else 'x'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opNpPZRprb2k"
   },
   "source": [
    "# Step 7: Split the Data\n",
    "\n",
    "Need to check for **Supervised** modelling:\n",
    "- Number of known cases or observations\n",
    "- Define the split in Training/Test or Training/Validation/Test and their proportions\n",
    "- Check for unbalanced classes and how to keep or avoid it when spliting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYJwBxVurb2k"
   },
   "source": [
    "# Step 7 (Answer): Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLb1z10Lrb2l"
   },
   "outputs": [],
   "source": [
    "## Create training and testing subsets\n",
    "test_size = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = test_size,\n",
    "                                                    random_state = 100666001,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7BTzOmmrb2m"
   },
   "source": [
    "# Step 8: Define and Fit Models\n",
    "\n",
    "Define the model and its hyper-parameters.\n",
    "\n",
    "Consider the parameters and hyper-parameters of each model at each (re)run and after checking the efficiency of a model against the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYsr-1_Zrb2n"
   },
   "source": [
    "# Step 8 (Answer): Define and Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM7h0CF0rb2n"
   },
   "source": [
    "### Use only Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYoaDdqjrb2o"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 10, random_state = 7)\n",
    "cart = DecisionTreeClassifier()\n",
    "cart.fit(X_train, y_train)\n",
    "results = cross_val_score(cart, X_train, y_train, cv = kfold)\n",
    "print('Decision Tree, Cross-Validation mean: %.4f' % results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffSxnxuxrb2p"
   },
   "source": [
    "### Use Boosting (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReQ1GLmlrb2q"
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "num_trees = 70\n",
    "kfold = model_selection.KFold(n_splits = 10, random_state = seed)\n",
    "model = AdaBoostClassifier(n_estimators = num_trees, random_state = seed)\n",
    "model.fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train, cv = kfold)\n",
    "print('AdaBoost, Cross-Validation mean: %.4f' % results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuOKBLwarb2r"
   },
   "source": [
    "# Step 9: Verify and Evaluate the Training Model\n",
    "- Use the **training** data to make predictions\n",
    "- Check for overfitting\n",
    "- What metrics are appropriate for the modelling approach used\n",
    "- For **Supervised** models:\n",
    "    - Check the **Training Results** with the **Training Predictions** during development\n",
    "- Analyse, modify the parameters and hyper-parameters and repeat (within reason) until the model does not improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1pQYrLprb2r"
   },
   "source": [
    "# Step 9 (Answers): Verify and Evaluate the Training Model\n",
    "Typical metrics for Classification problems are (some might be restricted to Binary Classification):\n",
    "- Feature Importance\n",
    "- Confusion Matrix\n",
    "- Accuracy\n",
    "- Precision/Recall\n",
    "- Receiver Operating Characteristic (ROC) and the Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnkwjXWTrb2s"
   },
   "outputs": [],
   "source": [
    "def show_summary_report(actual, prediction):\n",
    "\n",
    "    if isinstance(actual, pd.Series):\n",
    "        actual = actual.values\n",
    "    if actual.dtype.name == 'object':\n",
    "        actual = actual.astype(int)\n",
    "    if prediction.dtype.name == 'object':\n",
    "        prediction = prediction.astype(int)\n",
    "\n",
    "    accuracy_ = accuracy_score(actual, prediction)\n",
    "    precision_ = precision_score(actual, prediction)\n",
    "    recall_ = recall_score(actual, prediction)\n",
    "    roc_auc_ = roc_auc_score(actual, prediction)\n",
    "\n",
    "    print('Accuracy : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n",
    "    print('Precision: %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n",
    "    print('Recall   : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n",
    "    print('ROC AUC  : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n",
    "    print('-' * 107)\n",
    "    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    mat = confusion_matrix(actual, prediction)\n",
    "\n",
    "    # Precision/Recall\n",
    "    precision, recall, _ = precision_recall_curve(actual, prediction)\n",
    "    average_precision = average_precision_score(actual, prediction)\n",
    "    \n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(actual, prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (18, 6))\n",
    "    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Blues', ax = ax[0])\n",
    "\n",
    "    ax[0].set_title('Confusion Matrix')\n",
    "    ax[0].set_xlabel('True label')\n",
    "    ax[0].set_ylabel('Predicted label')\n",
    "    \n",
    "    # Precision/Recall\n",
    "    step_kwargs = {'step': 'post'}\n",
    "    ax[1].step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n",
    "    ax[1].fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n",
    "    ax[1].set_ylim([0.0, 1.0])\n",
    "    ax[1].set_xlim([0.0, 1.0])\n",
    "    ax[1].set_xlabel('Recall')\n",
    "    ax[1].set_ylabel('Precision')\n",
    "    ax[1].set_title('2-class Precision-Recall curve')\n",
    "\n",
    "    # ROC\n",
    "    ax[2].plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    ax[2].plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n",
    "    ax[2].set_xlim([0.0, 1.0])\n",
    "    ax[2].set_ylim([0.0, 1.0])\n",
    "    ax[2].set_xlabel('False Positive Rate')\n",
    "    ax[2].set_ylabel('True Positive Rate')\n",
    "    ax[2].set_title('Receiver Operating Characteristic')\n",
    "    ax[2].legend(loc = 'lower right')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return (accuracy_, precision_, recall_, roc_auc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avd1vhFrrb2t"
   },
   "outputs": [],
   "source": [
    "# Keep the results in a dataframe\n",
    "results = pd.DataFrame(columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "models = ['Decision Tree', 'AdaBoost']\n",
    "for i, m in enumerate([cart, model]):\n",
    "    print('*' * (len(models[i]) + 4))\n",
    "    print('* %s *' % models[i])\n",
    "    print('*' * (len(models[i]) + 4))\n",
    "    predictions = m.predict(X_train)\n",
    "    # show the report\n",
    "    accuracy_, precision_, recall_, roc_auc_ = show_summary_report(y_train, predictions)\n",
    "    # keep the results\n",
    "    results.loc[i] = {'Model': models[i], \n",
    "                      'Accuracy': accuracy_, \n",
    "                      'Precision': precision_,\n",
    "                      'Recall': recall_,\n",
    "                      'ROC_AUC': roc_auc_}\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUdKoJUvrb2u"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf5U39F4rb2v"
   },
   "source": [
    "# Step 10: Make Predictions and Evaluate the Test Model\n",
    "**NOTE**: **Do this only after not making any more improvements in the model**.\n",
    "\n",
    "- Use the **test** data to make predictions\n",
    "- For **Supervised** models:\n",
    "    - Check the **Test Results** with the **Test Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JSLv4YVrb2w"
   },
   "source": [
    "# Step 10 (Answer): Make Predictions and Evaluate the Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvVYeL-Srb2w"
   },
   "outputs": [],
   "source": [
    "# gbm.fit(xx_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xwsm0h2Krb2x"
   },
   "outputs": [],
   "source": [
    "accuracy_, precision_, recall_, roc_auc_ = show_summary_report(y_test, predictions)\n",
    "# keep the results\n",
    "results.loc[len(results)] = {\n",
    "    'Model': 'AdaBoost (with test)', \n",
    "    'Accuracy': accuracy_, \n",
    "    'Precision': precision_,\n",
    "    'Recall': recall_,\n",
    "    'ROC_AUC': roc_auc_}\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUUGEOSbrb2z"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69jJvmDDrb20"
   },
   "source": [
    "# Step 11: Solve the Problem or Answer the Question\n",
    "The results of an analysis or modelling can be used:\n",
    "- As part of a product or process, so the model can make predictions when new input data is available\n",
    "- As part of a report including text and charts to help understand the problem\n",
    "- As input for further questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BzfjoULrb20"
   },
   "source": [
    "# Step 11 (Answer): Solve the Problem or Answer the Question\n",
    "The model can be used to answer (predict) if a person is diagnosed with a **Benign** or **Malignant** cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsywZZHgrb20"
   },
   "outputs": [],
   "source": [
    "r = df.iloc[X_test.index[0]]\n",
    "for i, c in enumerate(df.columns[:-1]):\n",
    "    print('%-27s: %d' % (c, int(r[i])))\n",
    "print('%-27s: %s' % (df.columns[-1], 'Benign' if r[-1] == 2 else 'Malignant'))\n",
    "print('%-27s: %s' % ('Prediction', 'Benign' if predictions[0] == 0 else 'Malignant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2022 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IOD_Lab-7_2_2-Answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
