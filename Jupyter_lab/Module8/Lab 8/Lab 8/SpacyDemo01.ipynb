{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Demo01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "!pip install spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download english language model\n",
    "# There are 3 models: small, medium, large\n",
    "# The small model takes around 4 secs to load\n",
    "# The large model takes around 3 mins\n",
    "# Note that some functions work only in medium or large models\n",
    "\n",
    "%time !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intantiate nlp class for english\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation\n",
    "# doc = nlp('Drinking a glass of wine is good for your wellbeing!')\n",
    "doc = nlp('''Such an analysis can reveal features that are not easily visible from the variation in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation\n",
    "''')\n",
    "for token in doc:\n",
    "    print(f\"token:{token}\\t tag:{token.tag_}\\t\\tPOS:{token.pos_}\\t\\t text:'{token.text}' \\tlemma:{token.lemma_}\\t \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "# doc = nlp(\"He was born in Canberra, Australia in 14/1/1974\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text} \\t\\t type:{ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tag alongside the text\n",
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun-phrase chunking\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"Text:{chunk.text},\\t label:{chunk.label_},\\t root:{chunk.root.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar dependency tree parsing and visualisation\n",
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-base matcher\n",
    "# import spacy Matcher\n",
    "from spacy.matcher import Matcher\n",
    "# create a matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# define a function to extract full name\n",
    "def extract_full_name(text: str):\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('FULL_NAME', None, pattern)\n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        names.append(span.text)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find full name in sentence\n",
    "full_names = extract_full_name(\"I met John Richardson almost a year after Daniel Zhang married Lucy Khan\")\n",
    "print(f\"Full names: {full_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "filtered_text =[] \n",
    "nlp_text = nlp(text)\n",
    "token_list = []\n",
    "for token in nlp_text:\n",
    "    token_list.append(token.text)\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_text.append(word)\n",
    "print(f\"Token list: \\n{token_list}\")\n",
    "print(f\"\\nFiltered text: \\n{filtered_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vectors\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "for token in tokens:\n",
    "    print(f\"Token:\\t{token.text}, has vector:\\t{token.has_vector}, token.vector_norm, token.is_oov\")\n",
    "print(f\"\\nToken 1: {tokens[0]}\\n Vector:\\n{tokens[0].vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word similarity\n",
    "tokens = nlp(\"dog cat banana apple\")\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(f\"Token 1:\\t{token1.text},\\t token 2:{token2.text},\\t similarity:{token1.similarity(token2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analys with textBlob\n",
    "!pip install textBlob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect sentiment of a text\n",
    "# text = \"Textblob is amazingly simple to use. What great fun!\"\n",
    "text=\"I am so happy with my progress in the data science course\"\n",
    "textBlob = TextBlob(text)\n",
    "print(f\"{textBlob.sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2022 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
